{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Linaqruf/sd-notebook-collection/blob/main/sd-colab-toolkit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I. Installation"
      ],
      "metadata": {
        "id": "akFjukqz5PbB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 1.1 Install Dependencies\n",
        "#@markdown This will install required Python packages\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "root_dir = \"/content\"\n",
        "repo_dir = os.path.join(root_dir, \"sd-scripts\")\n",
        "models_dir = os.path.join(root_dir, \"models\")\n",
        "vaes_dir = os.path.join(root_dir, \"vae\")\n",
        "deps_dir = os.path.join(root_dir, \"deps\")\n",
        "tools_dir = os.path.join(repo_dir, \"tools\")\n",
        "\n",
        "ubuntu_deps_url = \"https://huggingface.co/Linaqruf/fast-repo/resolve/main/ubuntu-deps.zip\"\n",
        "ram_patch = \"https://huggingface.co/Linaqruf/fast-repo/resolve/main/ram_patch.zip\"\n",
        "\n",
        "def ubuntu_deps(url, dst):\n",
        "    os.makedirs(dst, exist_ok=True)\n",
        "    filename = os.path.basename(url)\n",
        "    !wget -q --show-progress {url}\n",
        "    with zipfile.ZipFile(filename, \"r\") as deps:\n",
        "        deps.extractall(dst)\n",
        "    !dpkg -i {dst}/*\n",
        "    os.remove(filename)\n",
        "    shutil.rmtree(dst)\n",
        "    \n",
        "def install_dependencies():\n",
        "    os.chdir(repo_dir)\n",
        "    !pip install --upgrade -r requirements.txt\n",
        "    !pip install --upgrade --no-cache-dir gdown\n",
        "\n",
        "def main():\n",
        "    for dir in [models_dir, vaes_dir]:\n",
        "        os.makedirs(dir, exist_ok=True)\n",
        "\n",
        "    !apt -y update -qq\n",
        "    !apt install libunwind8-dev -yqq\n",
        "    \n",
        "    for url in [ubuntu_deps_url, ram_patch]:\n",
        "        ubuntu_deps(url, deps_dir)\n",
        "    os.environ[\"LD_PRELOAD\"] = \"libtcmalloc.so\"\n",
        "\n",
        "    if not os.path.isdir(repo_dir):\n",
        "      !git clone https://github.com/kohya-ss/sd-scripts\n",
        "\n",
        "    install_dependencies()\n",
        "\n",
        "main()\n"
      ],
      "metadata": {
        "id": "0BicRIFqIjg0",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ## 2.2. Download Custom Model\n",
        "import os\n",
        "\n",
        "%store -r\n",
        "\n",
        "os.chdir(root_dir)\n",
        "\n",
        "# @markdown Fill in the URL fields with the links to the files you want to download. Separate multiple URLs with a comma.\n",
        "# @markdown Example: `url1, url2, url3`\n",
        "modelUrls = \"\"  # @param {'type': 'string'}\n",
        "\n",
        "def install(url):\n",
        "    base_name = os.path.basename(url)\n",
        "\n",
        "    if \"drive.google.com\" in url:\n",
        "        os.chdir(models_dir)\n",
        "        !gdown --fuzzy {url}\n",
        "    elif \"huggingface.co\" in url:\n",
        "        if \"/blob/\" in url:\n",
        "            url = url.replace(\"/blob/\", \"/resolve/\")\n",
        "        # @markdown Change this part with your own huggingface token if you need to download your private model\n",
        "        hf_token = \"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\"  # @param {type:\"string\"}\n",
        "        user_header = f'\"Authorization: Bearer {hf_token}\"'\n",
        "        !aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -d {models_dir} -o {base_name} {url}\n",
        "    else:\n",
        "        !aria2c --console-log-level=error --summary-interval=10 -c -x 16 -k 1M -s 16 -d {models_dir} {url}\n",
        "\n",
        "if modelUrls:\n",
        "    urls = modelUrls.split(\",\")\n",
        "    for url in urls:\n",
        "        install(url.strip())\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "A2NC0tm8vU-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ## 2.3. Download Available VAE (Optional)\n",
        "import os\n",
        "\n",
        "%store -r\n",
        "\n",
        "os.chdir(root_dir)\n",
        "\n",
        "vaes = {\n",
        "    \"none\": \"\",\n",
        "    \"anime.vae.pt\": \"https://huggingface.co/Linaqruf/personal-backup/resolve/main/vae/animevae.pt\",\n",
        "    \"waifudiffusion.vae.pt\": \"https://huggingface.co/hakurei/waifu-diffusion-v1-4/resolve/main/vae/kl-f8-anime.ckpt\",\n",
        "    \"stablediffusion.vae.pt\": \"https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.ckpt\",\n",
        "}\n",
        "install_vaes = []\n",
        "\n",
        "# @markdown Select one of the VAEs to download, select `none` for not download VAE:\n",
        "vae_name = \"anime.vae.pt\"  # @param [\"none\", \"anime.vae.pt\", \"waifudiffusion.vae.pt\", \"stablediffusion.vae.pt\"]\n",
        "\n",
        "if vae_name in vaes:\n",
        "    vae_url = vaes[vae_name]\n",
        "    if vae_url:\n",
        "        install_vaes.append((vae_name, vae_url))\n",
        "\n",
        "def install(vae_name, url):\n",
        "    hf_token = \"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\"\n",
        "    user_header = f'\"Authorization: Bearer {hf_token}\"'\n",
        "    !aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -d {vaes_dir} -o {vae_name} \"{url}\"\n",
        "\n",
        "def install_vae():\n",
        "    for vae in install_vaes:\n",
        "        install(vae[0], vae[1])\n",
        "\n",
        "install_vae()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "CZJLUBn3MgRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Model Conversion"
      ],
      "metadata": {
        "id": "WDPfF4uc5pd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "%store -r\n",
        "#@title ## 7.2. Model Pruner\n",
        "\n",
        "os.chdir(tools_dir)\n",
        "\n",
        "if not os.path.exists('prune.py'):\n",
        "    !wget https://raw.githubusercontent.com/lopho/stable-diffusion-prune/main/prune.py\n",
        "\n",
        "#@markdown Convert to Float16\n",
        "fp16 = False #@param {'type':'boolean'}\n",
        "#@markdown Use EMA for weights\n",
        "ema = False #@param {'type':'boolean'}\n",
        "#@markdown Strip CLIP weights\n",
        "no_clip = False #@param {'type':'boolean'}\n",
        "#@markdown Strip VAE weights\n",
        "no_vae = False #@param {'type':'boolean'}\n",
        "#@markdown Strip depth model weights\n",
        "no_depth = False #@param {'type':'boolean'}\n",
        "#@markdown Strip UNet weights\n",
        "no_unet = False #@param {'type':'boolean'}\n",
        "\n",
        "model_path = \"\" #@param {'type' : 'string'}\n",
        "\n",
        "config = {\n",
        "    \"fp16\": fp16,\n",
        "    \"ema\": ema,\n",
        "    \"no_clip\": no_clip,\n",
        "    \"no_vae\": no_vae,\n",
        "    \"no_depth\": no_depth,\n",
        "    \"no_unet\": no_unet,\n",
        "}\n",
        "\n",
        "suffixes = {\n",
        "    \"fp16\": \"-fp16\",\n",
        "    \"ema\": \"-ema\",\n",
        "    \"no_clip\": \"-no-clip\",\n",
        "    \"no_vae\": \"-no-vae\",\n",
        "    \"no_depth\": \"-no-depth\",\n",
        "    \"no_unet\": \"-no-unet\",\n",
        "}\n",
        "\n",
        "print(f\"Loading model from {model_path}\")\n",
        "\n",
        "dir_name = os.path.dirname(model_path)\n",
        "base_name = os.path.basename(model_path)\n",
        "output_name = base_name.split('.')[0]\n",
        "\n",
        "for option, suffix in suffixes.items():\n",
        "    if config[option]:\n",
        "        print(f\"Applying option {option}\")\n",
        "        output_name += suffix\n",
        "        \n",
        "output_name += '-pruned'\n",
        "output_path = os.path.join(dir_name, output_name + ('.ckpt' if model_path.endswith(\".ckpt\") else \".safetensors\"))\n",
        "\n",
        "args = \"\"\n",
        "for k, v in config.items():\n",
        "    if k.startswith(\"_\"):\n",
        "        args += f'\"{v}\" '\n",
        "    elif isinstance(v, str):\n",
        "        args += f'--{k}=\"{v}\" '\n",
        "    elif isinstance(v, bool) and v:\n",
        "        args += f\"--{k} \"\n",
        "    elif isinstance(v, float) and not isinstance(v, bool):\n",
        "        args += f\"--{k}={v} \"\n",
        "    elif isinstance(v, int) and not isinstance(v, bool):\n",
        "        args += f\"--{k}={v} \"\n",
        "\n",
        "final_args = f\"python3 prune.py {model_path} {output_path} {args}\"\n",
        "!{final_args}\n",
        "\n",
        "print(f\"Saving pruned model to {output_path}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "fb3rxuCaSYta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 7.1. Convert Diffusers to Checkpoint\n",
        "import os\n",
        "%store -r\n",
        "\n",
        "os.chdir(tools_dir)\n",
        "\n",
        "#@markdown ### Conversion Config\n",
        "model_to_load = \"\" #@param {'type': 'string'}\n",
        "model_to_save = os.path.splitext(model_to_load)[0]\n",
        "convert = \"checkpoint_to_diffusers\" #@param [\"diffusers_to_checkpoint\", \"checkpoint_to_diffusers\"] {'allow-input': false}\n",
        "v2 = True #@param {type:'boolean'}\n",
        "global_step = 0 #@param {'type': 'number'}\n",
        "epoch = 0 #@param {'type': 'number'}\n",
        "use_safetensors = True #@param {'type': 'boolean'}\n",
        "save_precision_as = \"--float\" #@param [\"--fp16\",\"--bf16\",\"--float\"] {'allow-input': false}\n",
        "\n",
        "#@markdown Additional option for diffusers\n",
        "feature_extractor = True #@param {'type': 'boolean'}\n",
        "safety_checker = True #@param {'type': 'boolean'}\n",
        "\n",
        "reference_model = \"stabilityai/stable-diffusion-2-1\" if v2 else \"runwayml/stable-diffusion-v1-5\" \n",
        "model_output = f\"{model_to_save}.safetensors\" if use_safetensors else f\"{model_to_save}.ckpt\"\n",
        "\n",
        "urls = [\n",
        "    (\"preprocessor_config.json\", \"https://huggingface.co/CompVis/stable-diffusion-safety-checker/resolve/main/preprocessor_config.json\"),\n",
        "    (\"config.json\", \"https://huggingface.co/CompVis/stable-diffusion-safety-checker/resolve/main/config.json\"),\n",
        "    (\"pytorch_model.bin\", \"https://huggingface.co/CompVis/stable-diffusion-safety-checker/resolve/main/pytorch_model.bin\"),\n",
        "]\n",
        "\n",
        "diffusers_to_sd_dict = {\n",
        "    \"_model_to_load\": model_to_load,\n",
        "    \"_model_to_save\": model_output,\n",
        "    \"global_step\": global_step,\n",
        "    \"epoch\": epoch,\n",
        "    \"save_precision_as\": save_precision_as,\n",
        "}\n",
        "\n",
        "sd_to_diffusers_dict = {\n",
        "    \"_model_to_load\": model_to_load,\n",
        "    \"_model_to_save\": model_to_save,\n",
        "    \"v2\": True if v2 else False,\n",
        "    \"v1\": True if not v2 else False,\n",
        "    \"global_step\": global_step,\n",
        "    \"epoch\": epoch,\n",
        "    \"fp16\": True if save_precision_as == \"fp16\" else False,\n",
        "    \"use_safetensors\": use_safetensors,\n",
        "    \"reference_model\": reference_model\n",
        "}\n",
        "\n",
        "def convert_dict(config):\n",
        "    args = \"\"\n",
        "    for k, v in config.items():\n",
        "        if k.startswith(\"_\"):\n",
        "            args += f'\"{v}\" '\n",
        "        elif isinstance(v, str):\n",
        "            args += f'--{k}=\"{v}\" '\n",
        "        elif isinstance(v, bool) and v:\n",
        "            args += f\"--{k} \"\n",
        "        elif isinstance(v, float) and not isinstance(v, bool):\n",
        "            args += f\"--{k}={v} \"\n",
        "        elif isinstance(v, int) and not isinstance(v, bool):\n",
        "            args += f\"--{k}={v} \"\n",
        "\n",
        "    return args\n",
        "\n",
        "def run_script(script_name, script_args):\n",
        "    !python {script_name} {script_args}\n",
        "\n",
        "def download(output, url, save_dir):\n",
        "    !aria2c --console-log-level=error --summary-interval=10 -c -x 16 -k 1M -s 16 -d '{save_dir}' -o '{output}' {url}\n",
        "\n",
        "diffusers_to_sd_args = convert_dict(diffusers_to_sd_dict)\n",
        "sd_to_diffusers_args = convert_dict(sd_to_diffusers_dict)\n",
        "\n",
        "if convert == \"diffusers_to_checkpoint\":\n",
        "    if model_to_load.endswith((\"ckpt\",\"safetensors\")):\n",
        "        print(f\"{os.path.basename(model_to_load)} is not in diffusers format\")\n",
        "    else:\n",
        "        run_script(\"convert_diffusers20_original_sd.py\", diffusers_to_sd_args)\n",
        "else:\n",
        "    if not model_to_load.endswith((\"ckpt\",\"safetensors\")):\n",
        "        print(f\"{os.path.basename(model_to_load)} is not in ckpt/safetensors format\")\n",
        "    else:     \n",
        "        run_script(\"convert_diffusers20_original_sd.py\", sd_to_diffusers_args)\n",
        "\n",
        "        if feature_extractor:\n",
        "            save_dir = os.path.join(model_to_save, \"feature_extractor\")\n",
        "            os.makedirs(save_dir, exist_ok=True)\n",
        "            output, url = urls[0]\n",
        "            download(output, url, save_dir)\n",
        "            \n",
        "        if safety_checker:\n",
        "            save_dir = os.path.join(model_to_save, \"safety_checker\")\n",
        "            os.makedirs(save_dir, exist_ok=True)\n",
        "            for output, url in urls[1:]:\n",
        "                download(output, url, save_dir)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "TdCb8_dSSzzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2.3. Replace VAE of Existing Model \n",
        "\n",
        "os.chdir(tools_dir)\n",
        "if not os.path.exists('merge_vae.py'):\n",
        "  !wget https://raw.githubusercontent.com/Linaqruf/kohya-trainer/main/tools/merge_vae.py\n",
        "\n",
        "#@markdown You need to input model ends with `.ckpt`, because `.safetensors` model won't work.\n",
        "\n",
        "target_model = \"\" #@param {'type': 'string'}\n",
        "target_vae = \"/content/vae/anime.vae.pt\" #@param {'type': 'string'}\n",
        "use_safetensors = False #@param {type:'boolean'}\n",
        "# get the base file name and directory\n",
        "base_name = os.path.basename(target_model)\n",
        "base_dir = os.path.dirname(target_model)\n",
        "\n",
        "# get the file name without extension\n",
        "file_name = os.path.splitext(base_name)[0]\n",
        "\n",
        "# create the new file name\n",
        "new_file_name = file_name + \"-vae-swapped\"\n",
        "\n",
        "# get the file extension\n",
        "file_ext = os.path.splitext(base_name)[1]\n",
        "\n",
        "# create the output file path\n",
        "output_model = os.path.join(base_dir, new_file_name + file_ext)\n",
        "\n",
        "!python merge_vae.py \\\n",
        "    {target_model} \\\n",
        "    {target_vae} \\\n",
        "    {output_model}\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "g2QfhhlfbGsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2.4. Convert CKPT-2-Safetensors\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from safetensors.torch import load_file, save_file\n",
        "from torch import load, save\n",
        "\n",
        "model_path = \"\" #@param {type: 'string'}\n",
        "\n",
        "def is_safetensors(path):\n",
        "  return os.path.splitext(path)[1].lower() == '.safetensors'\n",
        "\n",
        "def convert(model_path):\n",
        "  print(\"Loading model:\", os.path.basename(model_path))\n",
        "  \n",
        "  try:\n",
        "      with torch.no_grad():\n",
        "          print(\"Conversion in progress, please wait...\")\n",
        "          if is_safetensors(model_path):\n",
        "            model = load_file(model_path, device=\"cpu\")\n",
        "          else:\n",
        "            model = load(model_path, map_location=\"cpu\")\n",
        "          \n",
        "          if 'state_dict' in model:\n",
        "            sd = model['state_dict']\n",
        "          else:\n",
        "            sd = model\n",
        "\n",
        "          save_to = \".ckpt\" if is_safetensors(model_path) else \".safetensors\"\n",
        "          output = os.path.splitext(model_path)[0] + save_to\n",
        "\n",
        "          if is_safetensors(model_path):\n",
        "            save(sd, output)\n",
        "          else:\n",
        "            save_file(sd, output)\n",
        "\n",
        "      print(f'Successfully converted {os.path.basename(model_path)} to {os.path.basename(output)}')\n",
        "      print(f'located in this path : {output}')\n",
        "  except Exception as ex:\n",
        "      print(f'ERROR converting {os.path.basename(model_path)}: {ex}')\n",
        "\n",
        "  print('Done!')\n",
        "\n",
        "def main():\n",
        "  convert(model_path)\n",
        "main()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "L8WI17pmnlVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VIII. Deployment"
      ],
      "metadata": {
        "id": "nyIl9BhNXKUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ## 7.1. Upload Config\n",
        "from huggingface_hub import login\n",
        "from huggingface_hub import HfApi\n",
        "from huggingface_hub.utils import validate_repo_id, HfHubHTTPError\n",
        "\n",
        "# @markdown Login to Huggingface Hub\n",
        "# @markdown > Get **your** huggingface `WRITE` token [here](https://huggingface.co/settings/tokens)\n",
        "write_token = \"\"  # @param {type:\"string\"}\n",
        "# @markdown Fill this if you want to upload to your organization, or just leave it empty.\n",
        "orgs_name = \"\"  # @param{type:\"string\"}\n",
        "# @markdown If your model/dataset repo does not exist, it will automatically create it.\n",
        "repo_name = \"stolen\"  # @param{type:\"string\"}\n",
        "make_private = False  # @param{type:\"boolean\"}\n",
        "\n",
        "def authenticate(write_token):\n",
        "    login(write_token, add_to_git_credential=True)\n",
        "    api = HfApi()\n",
        "    return api.whoami(write_token), api\n",
        "\n",
        "def create_repo(api, user, orgs_name, repo_name, make_private=False):\n",
        "    if orgs_name == \"\":\n",
        "        repo_id = user[\"name\"] + \"/\" + repo_name.strip()\n",
        "    else:\n",
        "        repo_id = orgs_name + \"/\" + repo_name.strip()\n",
        "\n",
        "    try:\n",
        "        validate_repo_id(repo_id)\n",
        "        api.create_repo(repo_id=repo_id, private=make_private)\n",
        "        print(f\"Model repo '{repo_id}' didn't exist, creating repo\")\n",
        "    except HfHubHTTPError as e:\n",
        "        print(f\"Model repo '{repo_id}' exists, skipping create repo\")\n",
        "    \n",
        "    print(f\"Model repo '{repo_id}' link: https://huggingface.co/{repo_id}\\n\")\n",
        "    return repo_id\n",
        "\n",
        "user, api = authenticate(write_token)\n",
        "\n",
        "model_repo = create_repo(api, user, orgs_name, repo_name, make_private)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "QTXsM170GUpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.2. Upload with Huggingface Hub"
      ],
      "metadata": {
        "id": "Fuxghk8MnG6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ### 8.2.1. Upload Model\n",
        "from huggingface_hub import HfApi\n",
        "from huggingface_hub.utils import validate_repo_id, HfHubHTTPError\n",
        "from pathlib import Path\n",
        "\n",
        "api = HfApi()\n",
        "\n",
        "#@markdown This will be uploaded to model repo\n",
        "\n",
        "model_path = \"\" #@param {type :\"string\"}\n",
        "path_in_repo = \"\" #@param {type :\"string\"}\n",
        "revision = \"\" #@param {type :\"string\"}\n",
        "if revision:\n",
        "  api.create_branch(repo_id=model_repo, \n",
        "                branch=revision, \n",
        "                exist_ok=True)\n",
        "else:\n",
        "  revision = \"main\"\n",
        "project_name = os.path.basename(model_path)\n",
        "if project_name in [\".safetensors\", \"ckpt\", \"pt\"]:\n",
        "  project_name = os.path.split(model_path)[0]\n",
        "# @markdown Other Information\n",
        "commit_message = \"\"  # @param {type :\"string\"}\n",
        "\n",
        "if not commit_message:\n",
        "    commit_message = \"feat: upload \" + project_name + \" checkpoint\"\n",
        "\n",
        "if os.path.exists(model_path):\n",
        "  vae_exists = os.path.exists(os.path.join(model_path, 'vae'))\n",
        "  unet_exists = os.path.exists(os.path.join(model_path, 'unet'))\n",
        "  text_encoder_exists = os.path.exists(os.path.join(model_path, 'text_encoder'))\n",
        "    \n",
        "def upload_model(model_paths, is_folder :bool, commit_message):\n",
        "  path_obj = Path(model_paths)\n",
        "  trained_model = path_obj.parts[-1]\n",
        "  \n",
        "  if path_in_repo:\n",
        "    trained_model = path_in_repo\n",
        "    \n",
        "  if is_folder == True:\n",
        "    print(f\"Uploading {trained_model} to https://huggingface.co/\"+model_repo)\n",
        "    print(f\"Please wait...\")\n",
        "\n",
        "    if vae_exists and unet_exists and text_encoder_exists:\n",
        "      if not commit_message:\n",
        "        commit_message = f\"feat: upload diffusers version of {trained_model}\"\n",
        "\n",
        "      api.upload_folder(\n",
        "          folder_path=model_paths,\n",
        "          repo_id=model_repo,\n",
        "          revision=revision,\n",
        "          commit_message=commit_message,\n",
        "          ignore_patterns=\".ipynb_checkpoints\"\n",
        "          )\n",
        "    \n",
        "    else:\n",
        "      if not commit_message:\n",
        "        commit_message = f\"feat: upload {trained_model} checkpoint folder\"\n",
        "\n",
        "      api.upload_folder(\n",
        "          folder_path=model_paths,\n",
        "          path_in_repo=trained_model,\n",
        "          repo_id=model_repo,\n",
        "          revision=revision,\n",
        "          commit_message=commit_message,\n",
        "          ignore_patterns=\".ipynb_checkpoints\"\n",
        "          )\n",
        "    print(f\"Upload success, located at https://huggingface.co/\"+model_repo+\"/tree/main\\n\")\n",
        "  else: \n",
        "    print(f\"Uploading {trained_model} to https://huggingface.co/\"+model_repo)\n",
        "    print(f\"Please wait...\")\n",
        "    if not commit_message:\n",
        "      if model_paths.endswith(\".safetensors\"):\n",
        "        commit_message = f\"feat: upload safetensors version of {trained_model} \"\n",
        "      else:\n",
        "        commit_message = f\"feat: upload {trained_model} checkpoint\"\n",
        "            \n",
        "    api.upload_file(\n",
        "        path_or_fileobj=model_paths,\n",
        "        path_in_repo=trained_model,\n",
        "        repo_id=model_repo,\n",
        "        revision=revision,\n",
        "        commit_message=commit_message,\n",
        "        )\n",
        "        \n",
        "    print(f\"Upload success, located at https://huggingface.co/\"+model_repo+\"/blob/main/\"+trained_model+\"\\n\")\n",
        "      \n",
        "def upload():\n",
        "    if model_path.endswith((\".ckpt\", \".safetensors\", \".pt\")):\n",
        "      upload_model(model_path, False, commit_message)\n",
        "    else:\n",
        "      upload_model(model_path, True, commit_message)\n",
        "\n",
        "upload()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "CIeoJA-eO-8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.3. Upload with GIT (Alternative)"
      ],
      "metadata": {
        "id": "CKZpg4keWS5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ### 8.3.1. Clone Repository\n",
        "%cd /content/\n",
        "clone_model = True #@param {'type': 'boolean'}\n",
        "\n",
        "!git lfs install --skip-smudge\n",
        "!export GIT_LFS_SKIP_SMUDGE=1\n",
        "\n",
        "if clone_model:\n",
        "  !git clone https://huggingface.co/{model_repo} /content/{repo_name}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "6nBlrOrytO9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ### 8.3.2. Commit using Git \n",
        "import os\n",
        "\n",
        "os.chdir(root_dir)\n",
        "\n",
        "#@markdown Choose which repo you want to commit\n",
        "commit_model = True #@param {'type': 'boolean'}\n",
        "#@markdown #### Other Information\n",
        "commit_message = \"\" #@param {type :\"string\"}\n",
        "\n",
        "if not commit_message:\n",
        "  commit_message = f\"feat: upload {repo_name}\"\n",
        "\n",
        "!git config --global user.email \"example@mail.com\"\n",
        "!git config --global user.name \"example\"\n",
        "\n",
        "def commit(repo_folder, commit_message):\n",
        "  os.chdir(os.path.join(root_dir, repo_folder))\n",
        "  !git lfs install\n",
        "  !huggingface-cli lfs-enable-largefiles .\n",
        "  !git add .\n",
        "  !git commit -m \"{commit_message}\"\n",
        "  !git push\n",
        "\n",
        "commit(repo_name, commit_message)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "7bJev4PzOFFB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}